{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT6390 Project - Data Exploration snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful piece of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 1600498 text samples\n",
      "Index(['negative', 'neutral', 'positive'], dtype='object', name='target')\n",
      "[800177    139 800182]\n",
      "================================================================================\n",
      "                                                text    target\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative\n",
      "1  Got a headache :/ MC stop making music, you ca...  negative\n",
      "2  lol still worked like crazy lol  . lol Your la...  negative\n",
      "3  why won't netflix send me S. Darko? I know it'...  negative\n",
      "4  [ToZ] Clan Website offline  http://www.theoutl...  negative\n",
      "================================================================================\n",
      "positive    0.499958\n",
      "negative    0.499955\n",
      "neutral     0.000087\n",
      "Name: target, dtype: float64\n",
      "================================================================================\n",
      "**Only 139 'neutral' text samples**\n"
     ]
    }
   ],
   "source": [
    "def printStats(dataset):\n",
    "    print(f\"Total of {dataset.shape[0]} text samples\")\n",
    "    gr_by=dataset.groupby('target').count()\n",
    "    print(gr_by['text'].index)\n",
    "    print(gr_by['text'].values)\n",
    "    #print(gr_by)\n",
    "    print('='*80)\n",
    "    print(dataset.head())\n",
    "    print('='*80)\n",
    "    print(dataset['target'].value_counts(normalize=True))\n",
    "    \n",
    "\n",
    "    \n",
    "printStats(s140)\n",
    "print('='*80)\n",
    "s140_pos = s140[(s140.target == 'positive')]\n",
    "s140_neg = s140[(s140.target == 'negative')]\n",
    "s140_neut=s140.loc[s140['target']=='neutral']\n",
    "print(f\"**Only {s140_neut.shape[0]} 'neutral' text samples**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 6027 text samples\n",
      "Index(['N', 'No', 'Y', 'Yes'], dtype='object', name='target')\n",
      "[1041   58 2534  554]\n",
      "================================================================================\n",
      "                                                text  confidence target\n",
      "0  Global warming report urges governments to act...      1.0000    Yes\n",
      "1  Fighting poverty and global warming in Africa ...      1.0000    Yes\n",
      "2  Carbon offsets: How a Vatican forest failed to...      0.8786    Yes\n",
      "3  Carbon offsets: How a Vatican forest failed to...      1.0000    Yes\n",
      "4  URUGUAY: Tools Needed for Those Most Vulnerabl...      0.8087    Yes\n",
      "================================================================================\n",
      "Y      0.605207\n",
      "N      0.248627\n",
      "Yes    0.132314\n",
      "No     0.013852\n",
      "Name: target, dtype: float64\n",
      "================================================================================\n",
      "================================================================================\n",
      "Total of 50000 text samples\n",
      "Index(['negative', 'positive'], dtype='object', name='target')\n",
      "[25000 25000]\n",
      "================================================================================\n",
      "                                                text    target\n",
      "0  Story of a man who has unnatural feelings for ...  negative\n",
      "1  Airport '77 starts as a brand new luxury 747 p...  negative\n",
      "2  This film lacked something I couldn't put my f...  negative\n",
      "3  Sorry everyone,,, I know this is supposed to b...  negative\n",
      "4  When I was little my parents took me along to ...  negative\n",
      "================================================================================\n",
      "negative    0.5\n",
      "positive    0.5\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "printStats(cc)\n",
    "print('='*80)\n",
    "print('='*80)\n",
    "printStats(mr)\n",
    "\n",
    "#print('='*80)\n",
    "#s140_pos = s140[(s140.target == 'positive')]\n",
    "#s140_neg = s140[(s140.target == 'negative')]\n",
    "#s140_neut=s140.loc[s140['target']=='neutral']\n",
    "#print(f\"**Only {s140_neut.shape[0]} 'neutral' text samples**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 6027 text samples\n",
      "Index(['negative', 'positive'], dtype='object', name='target')\n",
      "[1099 3088]\n",
      "================================================================================\n",
      "                                                text  confidence    target  \\\n",
      "0  Global warming report urges governments to act...      1.0000  positive   \n",
      "1  Fighting poverty and global warming in Africa ...      1.0000  positive   \n",
      "2  Carbon offsets: How a Vatican forest failed to...      0.8786  positive   \n",
      "3  Carbon offsets: How a Vatican forest failed to...      1.0000  positive   \n",
      "4  URUGUAY: Tools Needed for Those Most Vulnerabl...      0.8087  positive   \n",
      "\n",
      "  old_target  \n",
      "0        Yes  \n",
      "1        Yes  \n",
      "2        Yes  \n",
      "3        Yes  \n",
      "4        Yes  \n",
      "================================================================================\n",
      "positive    0.737521\n",
      "negative    0.262479\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cc['old_target']=cc.target\n",
    "cc=cc.replace({'target': {'Yes': 'positive', 'Y': 'positive', 'No': 'negative', 'N': 'negative'}})\n",
    "printStats(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140=pd.read_pickle('data/s140_clean_28nov.pkl')\n",
    "mr=pd.read_pickle('data/mr_clean_28nov.pkl')\n",
    "cc=pd.read_pickle('data/cc_clean_28nov.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1600498\n",
      "mean           8\n",
      "std            4\n",
      "min            0\n",
      "25%            5\n",
      "50%            8\n",
      "75%           11\n",
      "max          118\n",
      "Name: length, dtype: int64\n",
      "================================================================================\n",
      "                                                text    target  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative   \n",
      "1  Got a headache :/ MC stop making music, you ca...  negative   \n",
      "2  lol still worked like crazy lol  . lol Your la...  negative   \n",
      "3  why won't netflix send me S. Darko? I know it'...  negative   \n",
      "4  [ToZ] Clan Website offline  http://www.theoutl...  negative   \n",
      "\n",
      "                                               lemma  length  \n",
      "0  [switchfoot, _link_, awww, 's, bummer, shoulda...      11  \n",
      "1  [got, headache, mc, stop, making, music, ca, n...      11  \n",
      "2  [lol, still, worked, like, crazy, lol, lol, la...      18  \n",
      "3  [wo, n't, netflix, send, s., darko, know, 's, ...      16  \n",
      "4              [toz, clan, website, offline, _link_]       5  \n"
     ]
    }
   ],
   "source": [
    "print(s140['length'].describe().astype('int64') )\n",
    "print('='*80)\n",
    "print(s140.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "def make_tfidf(df):\n",
    "    tf_vect=TfidfVectorizer(use_idf=False,stop_words=stopw)\n",
    "    tf=tf_vect.fit_transform(df.tolist())\n",
    "    \n",
    "    l=tf.shape[0]\n",
    "    \n",
    "    w_count=np.array(tf.sum(axis=0,))[0]/l\n",
    "    #wcm=w_count.max()\n",
    "    #w_count=w_count/wcm\n",
    "    wcr=w_count.argsort()[::-1].argsort()\n",
    "\n",
    "    \n",
    "    idf_vect=TfidfVectorizer(use_idf=True,stop_words=stopw)\n",
    "    idf=idf_vect.fit_transform(df.tolist())\n",
    "    \n",
    "    w_marker=np.array(idf.sum(axis=0))[0]/l\n",
    "    #wmm=w_marker.max()\n",
    "    #w_marker=w_marker/wmm\n",
    "    wmr=w_marker.argsort()[::-1].argsort()\n",
    "    \n",
    "    \n",
    "    # SRC -> https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n",
    "    feature_names=tf_vect.get_feature_names()\n",
    "    \n",
    "    tf_idf = pd.DataFrame(np.array([w_count, w_marker, wcr, wmr]).T, index=feature_names, columns=[\"tf\", \"idf\", \"rank_tf\", \"rank_idf\"])\n",
    "    tf_idf = tf_idf.sort_values(by=[\"tf\"],ascending=False)\n",
    "\n",
    "    tf_idf.rank_tf=tf_idf.rank_tf.astype('int64')\n",
    "    tf_idf.rank_idf=tf_idf.rank_idf.astype('int64')\n",
    "\n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=make_tfidf(s140['trimmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(use_idf=True,stop_words=stopw)\n",
    "tf_idf=tf_idf_vect.fit_transform(s140['trimmed'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_pos = s140[(s140.target == 'positive')]\n",
    "s140_neg = s140[(s140.target == 'negative')]\n",
    "s140_neut = s140[s140.target == 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=make_tfidf(s140_pos['trimmed'])\n",
    "neg=make_tfidf(s140_neg['trimmed'])\n",
    "neut=make_tfidf(s140_neut['trimmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.078189  0.029137        0         0\n",
      "day       0.022910  0.013212        1         1\n",
      "good      0.020175  0.011472        2         2\n",
      "_link_    0.017899  0.010193        3         4\n",
      "get       0.017192  0.009713        4         6\n",
      "go        0.016301  0.009873        5         5\n",
      "like      0.016169  0.009095        6         8\n",
      "work      0.015640  0.010381        7         3\n",
      "love      0.015129  0.008805        8        10\n",
      "today     0.014996  0.009305        9         7\n",
      "==================================================\n",
      "Positive documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.078936  0.029125        0         0\n",
      "good      0.027984  0.014872        1         1\n",
      "_link_    0.024132  0.012931        2         3\n",
      "day       0.024000  0.013735        3         2\n",
      "love      0.022817  0.012247        4         4\n",
      "thanks    0.017382  0.009025        5         6\n",
      "like      0.015616  0.008702        6         9\n",
      "lol       0.015239  0.008468        7        11\n",
      "get       0.015232  0.008723        8         8\n",
      "quot      0.014760  0.009860        9         5\n",
      "==================================================\n",
      "Negative documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.077442  0.029908        0         0\n",
      "work      0.022005  0.014225        1         1\n",
      "day       0.021823  0.013029        2         2\n",
      "go        0.020086  0.012114        3         3\n",
      "get       0.019153  0.010884        4         4\n",
      "today     0.016827  0.010469        5         5\n",
      "like      0.016724  0.009726        6         7\n",
      "want      0.015737  0.010047        7         6\n",
      "got       0.015613  0.009226        8        10\n",
      "miss      0.015508  0.009427        9         8\n",
      "==================================================\n",
      "Neutral documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_link_    0.182901  0.067740        0         0\n",
      "_number_  0.084310  0.046611        1         1\n",
      "safeway   0.045902  0.033673        2         2\n",
      "jquery    0.043330  0.033356        3         3\n",
      "eating    0.041964  0.031091        4         4\n",
      "twitter   0.027146  0.023399        5         5\n",
      "new       0.025899  0.020868        6         6\n",
      "night     0.025507  0.019951        7         9\n",
      "see       0.024423  0.019954        8         8\n",
      "nike      0.024278  0.019009        9        11\n"
     ]
    }
   ],
   "source": [
    "a=10\n",
    "b=50\n",
    "print(f'All documents top words\\n{all.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Positive documents top words\\n{pos.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Negative documents top words\\n{neg.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Neutral documents top words\\n{neut.head(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(ar1,ar2,ar3,t=1000):\n",
    "    \"\"\"Compare ar1 to ar2 and ar3\"\"\"\n",
    "\n",
    "    ar1_w=np.copy(ar1.index[:]).tolist()\n",
    "    \n",
    "    a=np.minimum(len(ar1),t)\n",
    "    b=np.minimum(len(ar2),t)\n",
    "    c=np.minimum(len(ar3),t)\n",
    "    i=0\n",
    "    \n",
    "    while i < a:\n",
    "        if (ar1.index[i] in ar2.index[:b]) or (ar1.index[i] in ar3.index[:c]):\n",
    "            del ar1_w[i]\n",
    "            a-=1\n",
    "            #ar1_w.pop(i)\n",
    "        else:\n",
    "            i+=1\n",
    "    \n",
    "    return ar1_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold=100\n",
    "key_pos=find_keywords(pos, neg, neut, treshold)\n",
    "key_neg=find_keywords(neg, pos, neut, treshold)\n",
    "key_neut=find_keywords(neut, neg, pos, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive documents top words\n",
      "['though', 'little', 'watch', 'ready', 'excited', 'always', 'sound', 'hi', 'school', 'made', 'pretty', 'long', 'lot', 'looking', 'ur', 'ya', 'hour', 'wow', 'cute', 'beautiful']\n",
      "==================================================\n",
      "Negative documents top words\n",
      "['soon', 'away', 'cold', 'life', 'little', 'left', 'wo', 'headache', 'guy', 'another', 'man', 'trying', 'great', 'omg', 'nothing', 'early', 'someone', 'wait', '_notascii_', 'poor']\n",
      "==================================================\n",
      "Neutral documents top words\n",
      "['saw', 'business', 'okay', 'com', 'sb', 'boy', 'store', 'food', 'like', 'ceo', 'ap', 'good', 'oh', 'old', 'fitness', 'need', 'battle', 'warner', 'read', 'breakfast']\n"
     ]
    }
   ],
   "source": [
    "a=20\n",
    "b=50\n",
    "\n",
    "print(f'Positive documents top words\\n{key_pos[:a]}')\n",
    "print('='*b)\n",
    "print(f'Negative documents top words\\n{key_neg[:a]}')\n",
    "print('='*b)\n",
    "print(f'Neutral documents top words\\n{key_neut[:a]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
