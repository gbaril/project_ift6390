{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT6390 Project - Remi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Useful piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140 = pd.read_csv('data/sentiment140.csv')\n",
    "cc = pd.read_csv('data/climatechange.csv')\n",
    "mr = pd.read_csv('data/moviereview.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140.to_pickle('data/s140.pkl')\n",
    "cc.to_pickle('data/cc.pkl')\n",
    "mr.to_pickle('data/mr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140 = pd.read_pickle('data/s140.pkl')\n",
    "cc = pd.read_pickle('data/cc.pkl')\n",
    "mr = pd.read_pickle('data/mr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 1600498 text sample\n",
      "================================================================================\n",
      "                                                text    target\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative\n",
      "1  Got a headache :/ MC stop making music, you ca...  negative\n",
      "2  lol still worked like crazy lol  . lol Your la...  negative\n",
      "3  why won't netflix send me S. Darko? I know it'...  negative\n",
      "4  [ToZ] Clan Website offline  http://www.theoutl...  negative\n",
      "================================================================================\n",
      "positive    0.499958\n",
      "negative    0.499955\n",
      "neutral     0.000087\n",
      "Name: target, dtype: float64\n",
      "================================================================================\n",
      "**Only 139 'neutral' sample comment**\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total of {s140.shape[0]} text samples\")\n",
    "print(s140.groupby('target').count())\n",
    "print('='*80)\n",
    "print(s140.head())\n",
    "print('='*80)\n",
    "print(s140['target'].value_counts(normalize=True))\n",
    "print('='*80)\n",
    "s140_pos = s140[(s140.target == 'positive')]\n",
    "s140_neg = s140[(s140.target == 'negative')]\n",
    "s140_neut=s140.loc[s140['target']=='neutral']\n",
    "print(f\"**Only {s140_neut.shape[0]} 'neutral' text samples**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize #creates arrays of words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#identifies words which are not adding semantic value to the sentence\n",
    "stopw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from re import sub\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class clean:\n",
    "    def url(df:pd.DataFrame) -> pd.DataFrame:\n",
    "        # SRC -> https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "        return df.str.replace('http\\S+|www.\\S+', '_link_', case=False)\n",
    "    \n",
    "    def url(s:str) -> str:\n",
    "        return sub('http\\S+|www.\\S+', '_link_', s, flags=re.IGNORECASE)\n",
    "    \n",
    "    def rm_repeat(w:str) -> str:\n",
    "        \"\"\"removes letters repeated 3+ times\"\"\"\n",
    "        return sub(r'(.)\\1{2,}', r'\\1', w)\n",
    "    \n",
    "    #create an array of the words contained in each comment _\n",
    "    #while removing strings representing numbers and stopwords\n",
    "    \n",
    "    def merge(v:np.ndarray) -> str:\n",
    "        return \"\".join(w+\" \" for w in v) #converts vector into string\n",
    "    \n",
    "    def convert(s:str) -> str:\n",
    "        \"\"\"Returns a vector representation of the sentence\"\"\"\n",
    "        s=clean.url(s)\n",
    "        s=clean.rm_repeat(s)\n",
    "        #v=[w for w in word_tokenize(s) if (w not in stopw and len(w) > 1)] # create words\n",
    "        v=word_tokenize(s)\n",
    "        return \"\".join(clean.to_ascii(w)+\" \" for w in v) #converts vector into string\n",
    "    \n",
    "    def to_ascii(w:str) -> str:\n",
    "        \"\"\"Keeps ascii-only characters and appends tokens representing \n",
    "        strings of digits and non-ascii characters\"\"\"\n",
    "        onlyascii= \"\".join(i for i in w.lower() if (ord(i) < 48 or (ord(i)> 57 and ord(i)<128)))        \n",
    "        return onlyascii + clean.notascii(w) + clean.onlynumber(w)\n",
    "    \n",
    "    def onlynumber(s:str) -> str:\n",
    "        \"\"\"Returns a '_number_' token to represent any string of digits\"\"\"\n",
    "        n=\"\".join(i for i in s if (ord(i) >= 48 and ord(i)<= 57 ))\n",
    "        if (n !=\"\"):\n",
    "            return \" _number_\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def notascii(s:str) -> str:\n",
    "        \"\"\"Map strings of non-ascii characters to '_notascii_' token\"\"\"\n",
    "        symbol= \"\".join(i for i in s if ord(i) >= 128)\n",
    "        if (symbol !=\"\"):\n",
    "            return \" _notascii_\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    \n",
    "    #SRC -> https://www.geeksforgeeks.org/implement-isnumber-function-in-python/\n",
    "    # Implementation of isNumber() function \n",
    "    def isNumber(s): \n",
    "        \"\"\"Considers strings of digits headed with sign characters\"\"\"\n",
    "\n",
    "        # handle for signed values\n",
    "        negative = False\n",
    "        if(s[0] =='-' or s[0] =='+'): \n",
    "            sign = True\n",
    "\n",
    "        if sign == True: \n",
    "            return clean.isNumber(s[1:]) #handles repeated signs recursively\n",
    "        else:\n",
    "            return s.isdigit()\n",
    "\n",
    "        ## try to convert the string to int \n",
    "        #try: \n",
    "        #    n = int(s) \n",
    "        #    return True\n",
    "        ## catch exception if cannot be converted \n",
    "        #except ValueError: \n",
    "        #    return False\n",
    "    \n",
    "\n",
    "    def normalize_text(s:str) -> str:\n",
    "        \"\"\"removes stop words, words of size 1, symbols and numbers\"\"\"\n",
    "        return [w for w in word_tokenize(clean.convert(s)) if (w not in stopw and len(w) > 1)]\n",
    "\n",
    "    def lemmatize(s:str) -> str:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        return [wnl.lemmatize(w) for w in clean.normalize_text(s)]\n",
    "\n",
    "    def stem(s:str) -> str:\n",
    "        ps  = PorterStemmer()\n",
    "        return [ps.stem(w) for w in clean.normalize_text(s)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140['lemma'] = s140['text'].apply(clean.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    target  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative   \n",
      "1  Got a headache :/ MC stop making music, you ca...  negative   \n",
      "2  lol still worked like crazy lol  . lol Your la...  negative   \n",
      "3  why won't netflix send me S. Darko? I know it'...  negative   \n",
      "4  [ToZ] Clan Website offline  http://www.theoutl...  negative   \n",
      "\n",
      "                                               lemma  length  \n",
      "0  [switchfoot, _link_, awww, 's, bummer, shoulda...      11  \n",
      "1  [got, headache, mc, stop, making, music, ca, n...      11  \n",
      "2  [lol, still, worked, like, crazy, lol, lol, la...      18  \n",
      "3  [wo, n't, netflix, send, s., darko, know, 's, ...      16  \n",
      "4              [toz, clan, website, offline, _link_]       5  \n"
     ]
    }
   ],
   "source": [
    "s140['length']=s140['lemma'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce cleaned dataframe for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140.to_pickle('data/s140_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140=pd.read_pickle('data/s140_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1600498\n",
      "mean           8\n",
      "std            4\n",
      "min            0\n",
      "25%            5\n",
      "50%            8\n",
      "75%           11\n",
      "max          118\n",
      "Name: length, dtype: int64\n",
      "================================================================================\n",
      "                                                text    target  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative   \n",
      "1  Got a headache :/ MC stop making music, you ca...  negative   \n",
      "2  lol still worked like crazy lol  . lol Your la...  negative   \n",
      "3  why won't netflix send me S. Darko? I know it'...  negative   \n",
      "4  [ToZ] Clan Website offline  http://www.theoutl...  negative   \n",
      "\n",
      "                                               lemma  length  \n",
      "0  [switchfoot, _link_, awww, 's, bummer, shoulda...      11  \n",
      "1  [got, headache, mc, stop, making, music, ca, n...      11  \n",
      "2  [lol, still, worked, like, crazy, lol, lol, la...      18  \n",
      "3  [wo, n't, netflix, send, s., darko, know, 's, ...      16  \n",
      "4              [toz, clan, website, offline, _link_]       5  \n"
     ]
    }
   ],
   "source": [
    "print(s140['length'].describe().astype('int64') )\n",
    "print('='*80)\n",
    "print(s140.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140['trimmed']=s140['lemma'].apply(clean.merge).apply(clean.rm_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "def make_tfidf(df):\n",
    "    tf_vect=TfidfVectorizer(use_idf=False,stop_words=stopw)\n",
    "    tf=tf_vect.fit_transform(df.tolist())\n",
    "    \n",
    "    l=tf.shape[0]\n",
    "    \n",
    "    w_count=np.array(tf.sum(axis=0,))[0]/l\n",
    "    #wcm=w_count.max()\n",
    "    #w_count=w_count/wcm\n",
    "    wcr=w_count.argsort()[::-1].argsort()\n",
    "\n",
    "    \n",
    "    idf_vect=TfidfVectorizer(use_idf=True,stop_words=stopw)\n",
    "    idf=idf_vect.fit_transform(df.tolist())\n",
    "    \n",
    "    w_marker=np.array(idf.sum(axis=0))[0]/l\n",
    "    #wmm=w_marker.max()\n",
    "    #w_marker=w_marker/wmm\n",
    "    wmr=w_marker.argsort()[::-1].argsort()\n",
    "    \n",
    "    \n",
    "    # SRC -> https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n",
    "    feature_names=tf_vect.get_feature_names()\n",
    "    \n",
    "    tf_idf = pd.DataFrame(np.array([w_count, w_marker, wcr, wmr]).T, index=feature_names, columns=[\"tf\", \"idf\", \"rank_tf\", \"rank_idf\"])\n",
    "    tf_idf = tf_idf.sort_values(by=[\"tf\"],ascending=False)\n",
    "\n",
    "    tf_idf.rank_tf=tf_idf.rank_tf.astype('int64')\n",
    "    tf_idf.rank_idf=tf_idf.rank_idf.astype('int64')\n",
    "\n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=make_tfidf(s140['trimmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(use_idf=True,stop_words=stopw)\n",
    "tf_idf=tf_idf_vect.fit_transform(s140['trimmed'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_pos = s140[(s140.target == 'positive')]\n",
    "s140_neg = s140[(s140.target == 'negative')]\n",
    "s140_neut = s140[s140.target == 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=make_tfidf(s140_pos['trimmed'])\n",
    "neg=make_tfidf(s140_neg['trimmed'])\n",
    "neut=make_tfidf(s140_neut['trimmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.078189  0.029137        0         0\n",
      "day       0.022910  0.013212        1         1\n",
      "good      0.020175  0.011472        2         2\n",
      "_link_    0.017899  0.010193        3         4\n",
      "get       0.017192  0.009713        4         6\n",
      "go        0.016301  0.009873        5         5\n",
      "like      0.016169  0.009095        6         8\n",
      "work      0.015640  0.010381        7         3\n",
      "love      0.015129  0.008805        8        10\n",
      "today     0.014996  0.009305        9         7\n",
      "==================================================\n",
      "Positive documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.078936  0.029125        0         0\n",
      "good      0.027984  0.014872        1         1\n",
      "_link_    0.024132  0.012931        2         3\n",
      "day       0.024000  0.013735        3         2\n",
      "love      0.022817  0.012247        4         4\n",
      "thanks    0.017382  0.009025        5         6\n",
      "like      0.015616  0.008702        6         9\n",
      "lol       0.015239  0.008468        7        11\n",
      "get       0.015232  0.008723        8         8\n",
      "quot      0.014760  0.009860        9         5\n",
      "==================================================\n",
      "Negative documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_number_  0.077442  0.029908        0         0\n",
      "work      0.022005  0.014225        1         1\n",
      "day       0.021823  0.013029        2         2\n",
      "go        0.020086  0.012114        3         3\n",
      "get       0.019153  0.010884        4         4\n",
      "today     0.016827  0.010469        5         5\n",
      "like      0.016724  0.009726        6         7\n",
      "want      0.015737  0.010047        7         6\n",
      "got       0.015613  0.009226        8        10\n",
      "miss      0.015508  0.009427        9         8\n",
      "==================================================\n",
      "Neutral documents top words\n",
      "                tf       idf  rank_tf  rank_idf\n",
      "_link_    0.182901  0.067740        0         0\n",
      "_number_  0.084310  0.046611        1         1\n",
      "safeway   0.045902  0.033673        2         2\n",
      "jquery    0.043330  0.033356        3         3\n",
      "eating    0.041964  0.031091        4         4\n",
      "twitter   0.027146  0.023399        5         5\n",
      "new       0.025899  0.020868        6         6\n",
      "night     0.025507  0.019951        7         9\n",
      "see       0.024423  0.019954        8         8\n",
      "nike      0.024278  0.019009        9        11\n"
     ]
    }
   ],
   "source": [
    "a=10\n",
    "b=50\n",
    "print(f'All documents top words\\n{all.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Positive documents top words\\n{pos.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Negative documents top words\\n{neg.head(a)}')\n",
    "print('='*b)\n",
    "print(f'Neutral documents top words\\n{neut.head(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(ar1,ar2,ar3,t=1000):\n",
    "    \"\"\"Compare ar1 to ar2 and ar3\"\"\"\n",
    "\n",
    "    ar1_w=np.copy(ar1.index[:]).tolist()\n",
    "    \n",
    "    a=np.minimum(len(ar1),t)\n",
    "    b=np.minimum(len(ar2),t)\n",
    "    c=np.minimum(len(ar3),t)\n",
    "    i=0\n",
    "    \n",
    "    while i < a:\n",
    "        if (ar1.index[i] in ar2.index[:b]) or (ar1.index[i] in ar3.index[:c]):\n",
    "            del ar1_w[i]\n",
    "            a-=1\n",
    "            #ar1_w.pop(i)\n",
    "        else:\n",
    "            i+=1\n",
    "    \n",
    "    return ar1_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold=100\n",
    "key_pos=find_keywords(pos, neg, neut, treshold)\n",
    "key_neg=find_keywords(neg, pos, neut, treshold)\n",
    "key_neut=find_keywords(neut, neg, pos, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive documents top words\n",
      "['though', 'little', 'watch', 'ready', 'excited', 'always', 'sound', 'hi', 'school', 'made', 'pretty', 'long', 'lot', 'looking', 'ur', 'ya', 'hour', 'wow', 'cute', 'beautiful']\n",
      "==================================================\n",
      "Negative documents top words\n",
      "['soon', 'away', 'cold', 'life', 'little', 'left', 'wo', 'headache', 'guy', 'another', 'man', 'trying', 'great', 'omg', 'nothing', 'early', 'someone', 'wait', '_notascii_', 'poor']\n",
      "==================================================\n",
      "Neutral documents top words\n",
      "['saw', 'business', 'okay', 'com', 'sb', 'boy', 'store', 'food', 'like', 'ceo', 'ap', 'good', 'oh', 'old', 'fitness', 'need', 'battle', 'warner', 'read', 'breakfast']\n"
     ]
    }
   ],
   "source": [
    "a=20\n",
    "b=50\n",
    "\n",
    "print(f'Positive documents top words\\n{key_pos[:a]}')\n",
    "print('='*b)\n",
    "print(f'Negative documents top words\\n{key_neg[:a]}')\n",
    "print('='*b)\n",
    "print(f'Neutral documents top words\\n{key_neut[:a]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train s140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (0.3.7)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from scikit-plot) (0.21.3)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from scikit-plot) (3.1.1)\n",
      "Requirement already satisfied: scipy>=0.9 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from scikit-plot) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from scikit-plot) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from scikit-learn>=0.18->scikit-plot) (1.16.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.0)\n",
      "Requirement already satisfied: six in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/rd/miniconda3/envs/UdeM/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-plot\n",
    "\n",
    "import scikitplot#.plotters as skplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "\n",
    "def TryModel(model, x, y):    \n",
    "    if True:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.10, random_state=42)\n",
    "        \n",
    "        clf = model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted = clf.predict(X_valid)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        acc=accuracy_score(y_valid, predicted)\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "    else:\n",
    "        scores = cross_val_score(model, x, y, cv=5)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    scikitplot.metrics.plot_confusion_matrix(y_valid, predicted,x_tick_rotation=90,figsize=(9,9))#, normalize=True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "x,y = tf_idf, s140['target']\n",
    "\n",
    "#TryModel(MultinomialNB(), x, y)\n",
    "#TryModel(linear_model.SGDClassifier(max_iter=1000, tol=1e-3), x, y)\n",
    "TryModel(MLPClassifier(alpha=0.025, max_iter=25,epsilon=1e-02,verbose=True), x, y)\n",
    "#TryModel(AdaBoostClassifier(DecisionTreeClassifier(max_depth=10)), x, y)\n",
    "#vch = VotingClassifier(estimators=[('NB25', clf1), ('SGDp', clf2), ('SGDlog', clf3),('NB35', clf4)], voting='hard')\n",
    "#vcs = VotingClassifier(estimators=[('NB10', clf1), ('SGD', clf2), ('SGDlog', clf3),('NB35', clf4)], voting='soft')\n",
    "#TryModel(svm.SVC(kernel='linear', C=1), x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClimateChange Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6027, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climatechange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'confidence', 'target'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climatechange.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>confidence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  confidence target\n",
       "0  Global warming report urges governments to act...      1.0000    Yes\n",
       "1  Fighting poverty and global warming in Africa ...      1.0000    Yes\n",
       "2  Carbon offsets: How a Vatican forest failed to...      0.8786    Yes\n",
       "3  Carbon offsets: How a Vatican forest failed to...      1.0000    Yes\n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...      0.8087    Yes"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climatechange.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC -> https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "text = climatechange.iloc[:, 0]\n",
    "\n",
    "# SRC -> https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "text = text.str.replace('http\\S+|www.\\S+', '[link]', case=False)\n",
    "\n",
    "climatechange.iloc[:, 0] = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y      0.605207\n",
       "N      0.248627\n",
       "Yes    0.132314\n",
       "No     0.013852\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climatechange['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist = climatechange[(climatechange.target == 'Y') | (climatechange.target == 'Yes')]\n",
    "not_exist = climatechange[(climatechange.target == 'N') | (climatechange.target == 'No')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        confidence\n",
      "count  3088.000000\n",
      "mean      0.821351\n",
      "std       0.178079\n",
      "min       0.343400\n",
      "25%       0.662800\n",
      "50%       0.806050\n",
      "75%       1.000000\n",
      "max       1.000000\n"
     ]
    }
   ],
   "source": [
    "print(exist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        confidence\n",
      "count  1099.000000\n",
      "mean      0.762216\n",
      "std       0.190782\n",
      "min       0.345100\n",
      "25%       0.650100\n",
      "50%       0.688000\n",
      "75%       1.000000\n",
      "max       1.000000\n"
     ]
    }
   ],
   "source": [
    "print(not_exist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('link', 2640), ('climat', 2001), ('chang', 1896), ('global', 1556), ('warm', 1503), ('rt', 522), ('the', 317), ('via', 281), ('new', 176), ('news', 147)]\n"
     ]
    }
   ],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (ps.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words, stop_words='english')\n",
    "\n",
    "data= exist.iloc[:,0].ravel()\n",
    "transformed_data =vectorizer.fit_transform(data)\n",
    "vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n",
    "\n",
    "print(sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('global', 911), ('warm', 904), ('link', 638), ('climat', 367), ('chang', 322), ('rt', 229), ('snow', 155), ('the', 131), ('tcot', 114), ('gore', 99)]\n"
     ]
    }
   ],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (ps.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words, stop_words='english')\n",
    "\n",
    "data= not_exist.iloc[:,0].ravel()\n",
    "transformed_data =vectorizer.fit_transform(data)\n",
    "vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n",
    "\n",
    "print(sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieReview Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviereview.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    target\n",
       "0  Story of a man who has unnatural feelings for ...  negative\n",
       "1  Airport '77 starts as a brand new luxury 747 p...  negative\n",
       "2  This film lacked something I couldn't put my f...  negative\n",
       "3  Sorry everyone,,, I know this is supposed to b...  negative\n",
       "4  When I was little my parents took me along to ...  negative"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviereview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC -> https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "text = moviereview.iloc[:, 0]\n",
    "\n",
    "text = text.str.replace('<br />', ' ', case=False)\n",
    "\n",
    "# SRC -> https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "text = text.str.replace('http\\S+|www.\\S+', '[link]', case=False)\n",
    "\n",
    "moviereview.iloc[:, 0] = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.5\n",
       "positive    0.5\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviereview['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = moviereview[(moviereview.target == 'positive')]\n",
    "bad = moviereview[(moviereview.target == 'negative')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text    target\n",
      "count                                               25000     25000\n",
      "unique                                              24881         1\n",
      "top     Loved today's show!!! It variety solely cookin...  positive\n",
      "freq                                                    5     25000\n"
     ]
    }
   ],
   "source": [
    "print(good.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text    target\n",
      "count                                               25000     25000\n",
      "unique                                              24696         1\n",
      "top     When got movie free job, along three similar m...  negative\n",
      "freq                                                    3     25000\n"
     ]
    }
   ],
   "source": [
    "print(bad.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 50894), ('the', 49203), ('movi', 44850), ('it', 32237), ('one', 28290), ('like', 20562), ('thi', 18000), ('time', 16630), ('good', 15262), ('see', 15141)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (ps.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words, stop_words='english')\n",
    "\n",
    "data= good.iloc[:,0].ravel()\n",
    "transformed_data =vectorizer.fit_transform(data)\n",
    "vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n",
    "\n",
    "print (sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movi', 58431), ('the', 49707), ('film', 44988), ('it', 31346), ('one', 27163), ('like', 24648), ('thi', 19196), ('make', 16221), ('even', 15440), ('time', 15335)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (ps.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words, stop_words='english')\n",
    "\n",
    "data= bad.iloc[:,0].ravel()\n",
    "transformed_data =vectorizer.fit_transform(data)\n",
    "vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n",
    "\n",
    "print (sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jordisaleilles/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_clf__C',\n",
       " 'param_clf__tol',\n",
       " 'param_tfidf__norm',\n",
       " 'param_vect__analyzer',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {\n",
    "    'vect__analyzer': [stemmed_words],\n",
    "    'tfidf__norm': ['l2'],\n",
    "    'clf__tol':[1e-1, 1e-2, 1e-3, 1e-4, 1e-5], \n",
    "    'clf__C':[100, 50, 20, 10, 1, 1e-1, 1e-2, 1e-3]\n",
    "    }\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer(analyzer=stemmed_words)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LinearSVC()),\n",
    "        ])\n",
    "\n",
    "clf = GridSearchCV(svc, param, cv=5)\n",
    "\n",
    "clf.fit(data_small.iloc[:,0],data_small.iloc[:,1])\n",
    "\n",
    "\n",
    "sorted(clf.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'clf__C': 0.1, 'clf__tol': 0.01, 'tfidf__norm': 'l2', 'vect__analyzer': <function stemmed_words at 0x150652950>}\n"
     ]
    }
   ],
   "source": [
    "print('Best Params: ', clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
